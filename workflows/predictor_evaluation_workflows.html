

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>4.11. Predictor Evaluation Workflows &mdash; Citrine Python 1.9.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.12. AI Engine Code Examples" href="code_examples.html" />
    <link rel="prev" title="4.10. Predictor Reports" href="predictor_reports.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.9.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">1. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_entry.html">2. GEMD Data Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_extraction.html">3. [ALPHA] Data Extraction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">4. AI Engine</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">4.1. Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="descriptors.html">4.2. Descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="descriptors.html#platform-vocabularies">4.3. Platform Vocabularies</a></li>
<li class="toctree-l2"><a class="reference internal" href="predictors.html">4.4. Predictors</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_spaces.html">4.5. Design Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="processors.html">4.6. Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_workflows.html">4.7. Design Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="scores.html">4.8. Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_sources.html">4.9. Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="predictor_reports.html">4.10. Predictor Reports</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.11. Predictor Evaluation Workflows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#predictor-evaluators">4.11.1. Predictor evaluators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">4.11.1.1. Cross-validation evaluator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#predictor-evaluation-metrics">4.11.2. Predictor evaluation metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#execution-and-results">4.11.3. Execution and results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example">4.11.4. Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#archive-and-restore">4.11.5. Archive and restore</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="code_examples.html">4.12. AI Engine Code Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../formulations_example.html">5. Formulations Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ/index.html">6. FAQ</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Citrine Python</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html"><span class="section-number">4. </span>AI Engine</a> &raquo;</li>
        
      <li><span class="section-number">4.11. </span>Predictor Evaluation Workflows</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/workflows/predictor_evaluation_workflows.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="predictor-evaluation-workflows">
<h1><span class="section-number">4.11. </span>Predictor Evaluation Workflows<a class="headerlink" href="#predictor-evaluation-workflows" title="Permalink to this headline">¶</a></h1>
<p>A <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a> evaluates the performance of a <a class="reference internal" href="predictors.html"><span class="doc">Predictor</span></a>.
Each workflow is composed of one or more <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.PredictorEvaluator" title="citrine.informatics.predictor_evaluator.PredictorEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluators</span></code></a>.</p>
<section id="predictor-evaluators">
<h2><span class="section-number">4.11.1. </span>Predictor evaluators<a class="headerlink" href="#predictor-evaluators" title="Permalink to this headline">¶</a></h2>
<p>A predictor evaluator defines a method to evaluate a predictor and any relevant configuration, e.g., k-fold cross-validation evaluation that specifies 3 folds.
Minimally, each predictor evaluator specifies a name, a set of predictor responses to evaluate and a set of metrics to compute for each response.
Evaluator names must be unique within a single workflow (more on that <a class="reference external" href="#execution-and-results">below</a>).
Responses are specified as a set of strings, where each string corresponds to a descriptor key of a predictor output.
Metrics are specified as a set of <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PredictorEvaluationMetric" title="citrine.informatics.predictor_evaluation_metrics.PredictorEvaluationMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationMetrics</span></code></a>.
The evaluator will only compute the subset of metrics valid for each response, so the top-level metrics defined by an evaluator should contain the union of all metrics computed across all responses.</p>
<section id="id1">
<h3><span class="section-number">4.11.1.1. </span>Cross-validation evaluator<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> performs k-fold cross-validation on a predictor.
Cross-validation can only be evaluated on predictors that define training data.
During cross-validation, the predictor’s training data is partitioned into k folds.
Each fold acts as the test set once, and the remaining k-1 folds are used as training data.
When the number of folds equals the number of training data points, the analysis is equivalent to leave-one-out cross-validation.
Metrics are computed by comparing the model’s predictions to observed values.
Where a metric is computed by taking an average over points in the test folds
the fold-wise average is reported as opposed to the point-wise average.</p>
<p>We recommend 5 folds and 3 trials as the default.
Decreasing the number of folds makes the cross-validation metrics less accurate, since the training sets are substantially reduced in size.
This can make the model appear to be less accurate than it actually is.
Conversely, increasing the number of folds makes the <em>uncertainty estimates</em> for cross-validation metrics less accurate (see warning below).
It makes the uncertainty larger, meaning it is more likely that two models will appear to have equivalent performance when in fact one is better than the other.
Increasing the number of trials results in more accurate cross-validation metrics, but takes longer because more models need to be trained.</p>
<p>This evaluator has an <code class="docutils literal notranslate"><span class="pre">ignore_when_grouping</span></code> argument, which can be used to keep similar materials together in a fold and thereby avoid unrealistically optimistic results.
Candidates with different values for ignored keys and identical values for all other predictor inputs will be placed in the same fold.
For example, if you are baking cakes with different ingredients and different oven temperatures and want to group together the data by the ingredients, then
you can set <code class="docutils literal notranslate"><span class="pre">ignore_when_grouping={&quot;oven</span> <span class="pre">temperature&quot;}</span></code>.
That way, two recipes that differ only in their oven temperature will always end up in the same fold.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There is no unbiased way to estimate the variance of a metric computed by cross-validation
(<a class="reference external" href="https://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf">source</a>).
Citrine Platform uses the bias-corrected fold-wise standard deviation, as this was found to be reasonably accurate, especially when the number of folds is &lt;= 5.
It is biased-upward, which makes the results more conservative.
If two models are found to differ significantly in their cross-validation metrics, that difference is likely repeatable.</p>
</div>
</section>
</section>
<section id="predictor-evaluation-metrics">
<h2><span class="section-number">4.11.2. </span>Predictor evaluation metrics<a class="headerlink" href="#predictor-evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<p>Predictor evaluation metrics are defined as part of a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.PredictorEvaluator" title="citrine.informatics.predictor_evaluator.PredictorEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluator</span></code></a>.
For all response types, the metric (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PVA" title="citrine.informatics.predictor_evaluation_metrics.PVA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PVA</span></code></a>) is available to compare predicted versus actual data.
Other available metrics depend on whether the response’s type is numeric or categorical.</p>
<p>For numeric responses, the following metrics are available:</p>
<blockquote>
<div><ul class="simple">
<li><p><em>Root-mean squared error</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RMSE" title="citrine.informatics.predictor_evaluation_metrics.RMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">RMSE</span></code></a>): square root of the average of the squared prediction error.
RMSE is a useful and popular statistical metric for model quality.
RMSE is optimized by least-squares regression.
It has the same units as the predicted quantity and corresponds to the standard deviation of the residuals not explained by the predictor.
Lower RMSE means the model is more accurate.</p></li>
<li><p><em>R^2</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RSquared" title="citrine.informatics.predictor_evaluation_metrics.RSquared"><code class="xref py py-class docutils literal notranslate"><span class="pre">RSquared</span></code></a>): 1 - (mean squared error / variance of data).
More precisely known as the “fraction of variance explained,” this metric is equal to the coefficient of determination calculated with respect to the line <code class="docutils literal notranslate"><span class="pre">predicted</span> <span class="pre">=</span> <span class="pre">actual</span></code>.
Hence it is commonly referred to as “R^2,” but unlike R^2 in the context of linear regression, this metric can be negative.
Positive values mean that the model captures some of the variation across the training data, and it can be used to drive Sequential Learning.
A value of 1.0 indicates a perfect model.
R^2 is evaluated over all cross-validation folds, hence no uncertainty is calculated for the metric, though the value will vary slightly if cross validation is re-run.</p></li>
<li><p><em>Non-dimensional error</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.NDME" title="citrine.informatics.predictor_evaluation_metrics.NDME"><code class="xref py py-class docutils literal notranslate"><span class="pre">NDME</span></code></a>): RMSE divided by the standard deviation of the observed values in the test set.
(If training and test set are drawn from the same distribution, the standard deviation of the test set observed values is equivalent to the RMSE of a model that always predicts the mean of the observed values).
NDME is a useful non-dimensional model quality metric.
A value of NDME == 0 is a perfect model.
If NDME == 1, then the model is uninformative.
Generally, models with NDME &lt; 0.9 can be used in a design workflow.</p></li>
<li><p><em>Standard residual</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.StandardRMSE" title="citrine.informatics.predictor_evaluation_metrics.StandardRMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardRMSE</span></code></a>) is the root mean square of standardized errors (prediction errors divided by their predicted uncertainty).
1.0 is perfectly calibrated.
Standard residual provides a way to determine whether uncertainty estimates are well-calibrated for this model.
Residuals are calculated using <code class="docutils literal notranslate"><span class="pre">(Predicted</span> <span class="pre">-</span> <span class="pre">Actual)/(Uncertainty</span> <span class="pre">Estimate)</span></code>.
A value below 1 indicates the model is underconfident, i.e., actual values are within predicted error bars, on average.
A value over 1 indicates the model is overconfident, i.e., actual values fall outside predicted error bars, on average.</p></li>
<li><p><em>Coverage probability</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.CoverageProbability" title="citrine.informatics.predictor_evaluation_metrics.CoverageProbability"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoverageProbability</span></code></a>) is the fraction of observations for which the magnitude of the error is within a confidence interval of a given coverage level.
The default coverage level is 0.683, corresponding to one standard deviation.
The coverage level and coverage probability must both be between 0 and 1.0.
If the coverage probability is greater than the coverage level then the model is under-confident, and if the coverage probability is less than the coverage level then the model is over-confident.
While standard residual is weighted towards the outside of the residual distribution (because it looks like a 2-norm), coverage probability gives information about the center of the residual distribution.</p></li>
</ul>
</div></blockquote>
<p>For categorical responses, performance metrics include the area under the receiver operating characteristic (ROC) curve (if there are 2 categories) and the F1 score.</p>
<ul class="simple">
<li><p>Area under the ROC curve (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.AreaUnderROC" title="citrine.informatics.predictor_evaluation_metrics.AreaUnderROC"><code class="xref py py-class docutils literal notranslate"><span class="pre">AreaUnderROC</span></code></a>) represents the ability of the model to correctly distinguish samples between two categories.
If AUC == 1.0, all samples are classified correctly.
If AUC == 0.5, the model cannot distinguish between the two categories.
If AUC == 0.0, all samples are classified incorrectly.</p></li>
<li><p>Support-weighted F1 score (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.F1" title="citrine.informatics.predictor_evaluation_metrics.F1"><code class="xref py py-class docutils literal notranslate"><span class="pre">F1</span></code></a>) is calculated from averaged precision and recall of the model, weighted by the in-class fraction of true positives according to the formula <code class="docutils literal notranslate"><span class="pre">2.0</span> <span class="pre">*</span> <span class="pre">precision</span> <span class="pre">*</span> <span class="pre">recall</span> <span class="pre">/</span> <span class="pre">(precision</span> <span class="pre">+</span> <span class="pre">recall)</span> <span class="pre">*</span> <span class="pre">fraction_true_positives</span></code> summed over each class.
Scores are bounded by 0 and 1.
At a value of 1, the model has perfect precision and recall.</p></li>
</ul>
</section>
<section id="execution-and-results">
<span id="id2"></span><h2><span class="section-number">4.11.3. </span>Execution and results<a class="headerlink" href="#execution-and-results" title="Permalink to this headline">¶</a></h2>
<p>Triggering a Predictor Evaluation Workflow produces a <code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationExecution</span></code>.
This execution allows you to track the progress using its <code class="docutils literal notranslate"><span class="pre">status</span></code> and <code class="docutils literal notranslate"><span class="pre">status_info</span></code> properties.
The <code class="docutils literal notranslate"><span class="pre">status</span></code> can be one of <code class="docutils literal notranslate"><span class="pre">INPROGRESS</span></code>, <code class="docutils literal notranslate"><span class="pre">READY</span></code>, or <code class="docutils literal notranslate"><span class="pre">FAILED</span></code>.
Information about the execution status, e.g., warnings or reasons for failure, can be accessed via <code class="docutils literal notranslate"><span class="pre">status_info</span></code>.</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">status</span></code> is <code class="docutils literal notranslate"><span class="pre">READY</span></code>, results for each evaluation defined as part of the workflow can be accessed using the <code class="docutils literal notranslate"><span class="pre">results</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">execution</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="s1">&#39;evaluator_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>or by indexing into the execution object directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">execution</span><span class="p">[</span><span class="s1">&#39;evaluator_name&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Both methods return a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.PredictorEvaluationResult" title="citrine.informatics.predictor_evaluation_result.PredictorEvaluationResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationResult</span></code></a>.</p>
<p>Each evaluator defines its own result.
A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> returns a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.CrossValidationResult" title="citrine.informatics.predictor_evaluation_result.CrossValidationResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationResult</span></code></a>, for example.
All predictor evaluation results contain a reference to the evaluator that created the result, the set of responses that were evaluated and the metrics that were computed.</p>
<p>Values associated with computed metrics can be accessed by response key:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response_metrics</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;response_key&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>This returns a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.ResponseMetrics" title="citrine.informatics.predictor_evaluation_result.ResponseMetrics"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResponseMetrics</span></code></a> object.
This object contains all metrics that were computed for the <code class="docutils literal notranslate"><span class="pre">response_key</span></code>.
These metrics can be listed using <code class="docutils literal notranslate"><span class="pre">list(response_metrics)</span></code>,
and the value associated with a specific metric can be accessed by the metric itself, e.g., <code class="docutils literal notranslate"><span class="pre">response_metrics[RMSE()]</span></code> to retrieve the root-mean squared error.</p>
<p>With the exception of predicted vs. actual data, all metric values are returned as a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.RealMetricValue" title="citrine.informatics.predictor_evaluation_result.RealMetricValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">RealMetricValue</span></code></a>.
This object defines the properties <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">standard_error</span></code>.
The latter optionally returns a float if the evaluation was configured with enough trials to allow <code class="docutils literal notranslate"><span class="pre">standard_error</span></code> to be computed.
(A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> requires at least 3 trials to compute <code class="docutils literal notranslate"><span class="pre">standard_error</span></code>.)</p>
<p>Predicted vs. actual data (<code class="docutils literal notranslate"><span class="pre">response_metrics[PVA()]</span></code>) is returned as a list of predicted vs. actual data points.
Each data point defines properties <code class="docutils literal notranslate"><span class="pre">uuid</span></code>, <code class="docutils literal notranslate"><span class="pre">identifiers</span></code>, <code class="docutils literal notranslate"><span class="pre">trial</span></code>, <code class="docutils literal notranslate"><span class="pre">fold</span></code>, <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">uuid</span></code> and <code class="docutils literal notranslate"><span class="pre">identifiers</span></code> allow you to link a predicted vs. actual data point to the corresponding row in the <a class="reference internal" href="predictors.html#predictors"><span class="std std-ref">Predictor</span></a>’s <a class="reference internal" href="data_sources.html#data-sources"><span class="std std-ref">Data Source</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trial</span></code> and <code class="docutils literal notranslate"><span class="pre">fold</span></code> return the respective index assigned during the evaluation.</p></li>
<li><p>The form of <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code> data depends on whether the response is numeric or categorical.
For numeric responses, <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code> return a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.RealMetricValue" title="citrine.informatics.predictor_evaluation_result.RealMetricValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">RealMetricValue</span></code></a> which reports mean and standard error associated the data point.
For categorical responses, class probabilities are returned as a mapping from each class name (as a string) to its relative frequency (as a float).</p></li>
</ul>
</div></blockquote>
</section>
<section id="example">
<h2><span class="section-number">4.11.4. </span>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>The following demonstrates how to create a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a>, add it to a <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a>, and use it to evaluate a <a class="reference internal" href="../reference/citrine.informatics.predictors.predictor.html#citrine.informatics.predictors.predictor.Predictor" title="citrine.informatics.predictors.predictor.Predictor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Predictor</span></code></a>.</p>
<p>The predictor we’ll evaluate is defined below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.data_sources</span> <span class="kn">import</span> <span class="n">CSVDataSource</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.descriptors</span> <span class="kn">import</span> <span class="n">RealDescriptor</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.predictors</span> <span class="kn">import</span> <span class="n">SimpleMLPredictor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">RealDescriptor</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">RealDescriptor</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">data_source</span> <span class="o">=</span> <span class="n">CSVDataSource</span><span class="p">(</span>
    <span class="n">file_link</span><span class="o">=</span><span class="n">file</span><span class="p">,</span> <span class="c1"># path to CSV that contains training data for x and y</span>
    <span class="n">column_definitions</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">SimpleMLPredictor</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y predictor&#39;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s1">&#39;predicts y given x&#39;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span>
    <span class="n">latent_variables</span><span class="o">=</span><span class="p">[],</span>
    <span class="n">training_data</span><span class="o">=</span><span class="p">[</span><span class="n">data_source</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This predictor expects <code class="docutils literal notranslate"><span class="pre">x</span></code> as an input and predicts <code class="docutils literal notranslate"><span class="pre">y</span></code>.
Training data is provided by a <a class="reference internal" href="../reference/citrine.informatics.data_sources.html#citrine.informatics.data_sources.CSVDataSource" title="citrine.informatics.data_sources.CSVDataSource"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVDataSource</span></code></a> that assumes <code class="docutils literal notranslate"><span class="pre">filename</span></code> represents the path to a CSV that contains <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>Next, create a project and register the predictor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">citrine.jobs.waiting</span> <span class="kn">import</span> <span class="n">wait_while_validating</span>
<span class="kn">from</span> <span class="nn">citrine.seeding.find_or_create</span> <span class="kn">import</span> <span class="n">find_or_create_project</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Citrine</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;CITRINE_API_KEY&#39;</span><span class="p">))</span>
<span class="n">project</span> <span class="o">=</span> <span class="n">find_or_create_project</span><span class="p">(</span><span class="n">project_collection</span><span class="o">=</span><span class="n">client</span><span class="o">.</span><span class="n">projects</span><span class="p">,</span> <span class="n">project_name</span><span class="o">=</span><span class="s1">&#39;example project&#39;</span><span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">project</span><span class="o">.</span><span class="n">predictors</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
<span class="n">wait_while_validating</span><span class="p">(</span><span class="n">collection</span><span class="o">=</span><span class="n">project</span><span class="o">.</span><span class="n">predictors</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">predictor</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example we’ll create a cross-validation evaluator for the response <code class="docutils literal notranslate"><span class="pre">y</span></code> with 8 folds and 3 trials and request metrics for root-mean square error (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RMSE" title="citrine.informatics.predictor_evaluation_metrics.RMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">RMSE</span></code></a>) and predicted vs. actual data (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PVA" title="citrine.informatics.predictor_evaluation_metrics.PVA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PVA</span></code></a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here we’re performing cross-validation on an output, but latent variables are valid cross-validation responses as well.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.predictor_evaluator</span> <span class="kn">import</span> <span class="n">CrossValidationEvaluator</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.predictor_evaluation_metrics</span> <span class="kn">import</span> <span class="n">RMSE</span><span class="p">,</span> <span class="n">PVA</span>

<span class="n">evaluator</span> <span class="o">=</span> <span class="n">CrossValidationEvaluator</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cv&#39;</span><span class="p">,</span>
    <span class="n">n_folds</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">responses</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;y&#39;</span><span class="p">},</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="n">RMSE</span><span class="p">(),</span> <span class="n">PVA</span><span class="p">()}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Then add the evaluator to a <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a>, register it with your project, and wait for validation to finish:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.workflows</span> <span class="kn">import</span> <span class="n">PredictorEvaluationWorkflow</span>

<span class="n">workflow</span> <span class="o">=</span> <span class="n">PredictorEvaluationWorkflow</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;workflow that evaluates y&#39;</span><span class="p">,</span>
    <span class="n">evaluators</span><span class="o">=</span><span class="p">[</span><span class="n">evaluator</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">workflow</span> <span class="o">=</span> <span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">workflow</span><span class="p">)</span>
<span class="n">wait_while_validating</span><span class="p">(</span><span class="n">collection</span><span class="o">=</span><span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">workflow</span><span class="p">)</span>
</pre></div>
</div>
<p>Trigger the workflow against a predictor to start an execution.
Then wait for the results to be ready:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.jobs.waiting</span> <span class="kn">import</span> <span class="n">wait_while_executing</span>

<span class="n">execution</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">executions</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
<span class="n">wait_while_executing</span><span class="p">(</span><span class="n">collection</span><span class="o">=</span><span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_executions</span><span class="p">,</span> <span class="n">execution</span><span class="o">=</span><span class="n">execution</span><span class="p">,</span> <span class="n">print_status_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, load the results and inspect the metrics and their computed values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the results computed by the CV evaluator defined above</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">execution</span><span class="p">[</span><span class="n">evaluator</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

<span class="c1"># load results for y</span>
<span class="n">y_results</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>

<span class="c1"># listing the results should return the metrics we requested: RMSE and PVA</span>
<span class="n">computed_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">computed_metrics</span><span class="p">)</span> <span class="c1"># [&#39;rmse&#39;, &#39;predicted_vs_actual&#39;]</span>

<span class="c1"># access RMSE and print the mean and standard error</span>
<span class="n">y_rmse</span> <span class="o">=</span> <span class="n">y_results</span><span class="p">[</span><span class="n">RMSE</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;RMSE: mean = </span><span class="si">{</span><span class="n">y_rmse</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">, standard error = </span><span class="si">{</span><span class="n">y_rmse</span><span class="o">.</span><span class="n">standard_error</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># access PVA:</span>
<span class="n">y_pva</span> <span class="o">=</span> <span class="n">y_results</span><span class="p">[</span><span class="n">PVA</span><span class="p">()]</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pva</span><span class="p">))</span> <span class="c1"># this should equal the num_trials * num_folds * num_rows</span>
                  <span class="c1"># where num_rows == the number of rows in the data source</span>

<span class="c1"># inspect the first data point</span>
<span class="n">pva_data_point</span> <span class="o">=</span> <span class="n">y_pva</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># print trial and fold indices</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pva_data_point</span><span class="o">.</span><span class="n">trial</span><span class="p">)</span> <span class="c1"># should be == 1 since trials are 1-indexed,</span>
                            <span class="c1"># and this it the first data point</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pva_data_point</span><span class="o">.</span><span class="n">fold</span><span class="p">)</span> <span class="c1"># should also be == 1</span>

<span class="c1"># inspect predicted and actual values</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">pva_data_point</span><span class="o">.</span><span class="n">predicted</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted = </span><span class="si">{</span><span class="n">predicted</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="n">predicted</span><span class="o">.</span><span class="n">standard_error</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">pva_data_point</span><span class="o">.</span><span class="n">actual</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;actual = </span><span class="si">{</span><span class="n">actual</span><span class="o">.</span><span class="n">mean</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="n">actual</span><span class="o">.</span><span class="n">standard_error</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="archive-and-restore">
<h2><span class="section-number">4.11.5. </span>Archive and restore<a class="headerlink" href="#archive-and-restore" title="Permalink to this headline">¶</a></h2>
<p>Both <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflows</span></code></a> and <code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationExecutions</span></code> can be archived and restored.
To archive a workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="o">.</span><span class="n">archive</span><span class="p">(</span><span class="n">workflow</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
</pre></div>
</div>
<p>and to archive all executions associated with a predictor evaluation workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">execution</span> <span class="ow">in</span> <span class="n">workflow</span><span class="o">.</span><span class="n">executions</span><span class="o">.</span><span class="n">list</span><span class="p">():</span>
    <span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_executions</span><span class="o">.</span><span class="n">archive</span><span class="p">(</span><span class="n">execution</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
</pre></div>
</div>
<p>To restore a workflow or execution, simply replace <code class="docutils literal notranslate"><span class="pre">archive</span></code> with <code class="docutils literal notranslate"><span class="pre">restore</span></code> in the code above.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="code_examples.html" class="btn btn-neutral float-right" title="4.12. AI Engine Code Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="predictor_reports.html" class="btn btn-neutral float-left" title="4.10. Predictor Reports" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019, Citrine Informatics.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>

  
  
    
   

</body>
</html>