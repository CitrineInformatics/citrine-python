
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>4.11. Predictor Evaluation Workflows &#8212; Citrine Python 0.106.0 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.12. [DEPRECATED] Performance Workflows" href="performance_workflows.html" />
    <link rel="prev" title="4.10. Predictor Reports" href="predictor_reports.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="predictor-evaluation-workflows">
<h1>4.11. Predictor Evaluation Workflows<a class="headerlink" href="#predictor-evaluation-workflows" title="Permalink to this headline">¶</a></h1>
<p>A <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a> evaluates the performance of a <a class="reference internal" href="predictors.html"><span class="doc">Predictor</span></a>.
Each workflow is composed of one or more <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.PredictorEvaluator" title="citrine.informatics.predictor_evaluator.PredictorEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluators</span></code></a>.</p>
<div class="section" id="predictor-evaluators">
<h2>4.11.1. Predictor evaluators<a class="headerlink" href="#predictor-evaluators" title="Permalink to this headline">¶</a></h2>
<p>A predictor evaluator defines a method to evaluate a predictor and any relevant configuration, e.g., k-fold cross-validation evaluation that specifies 3 folds.
Minimally, each predictor evaluator specifies a name, a set of predictor responses to evaluate and a set of metrics to compute for each response.
Evaluator names must be unique within a single workflow (more on that <a class="reference external" href="#execution-and-results">below</a>).
Responses are specified as a set of strings, where each string corresponds to a descriptor key of a predictor output.
Metrics are specified as a set of <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PredictorEvaluationMetric" title="citrine.informatics.predictor_evaluation_metrics.PredictorEvaluationMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationMetrics</span></code></a>.
The evaluator will only compute the subset of metrics valid for each response, so the top-level metrics defined by an evaluator should contain the union of all metrics computed across all responses.</p>
<div class="section" id="cross-validation-evaluator">
<h3>4.11.1.1. Cross-validation evaluator<a class="headerlink" href="#cross-validation-evaluator" title="Permalink to this headline">¶</a></h3>
<p>A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> performs k-fold cross-validation on a predictor.
Cross-validation can only be evaluated on predictors that define training data.
During cross-validation, the predictor’s training data is partitioned into k folds.
Each fold acts as the test set once, and the remaining k-1 folds are used as training data.
When the number of folds equals the number of training data points, the analysis is equivalent to leave-one-out cross-validation.
Metrics are computed by comparing the model’s predictions to observed values.
Where a metric is computed by taking an average over points in the test folds
the fold-wise average is reported as opposed to the point-wise average.</p>
<p>In addition to a name, set of responses to validate, trials, folds and metrics to compute, this evaluator defines a set of descriptor keys to ignore when grouping.
Candidates with different values for ignored keys and identical values for all other predictor inputs will be placed in the same fold.
For example, if you are baking cakes with different ingredients and different oven temperatures and want to group together the data by the ingredients, then
you can set <cite>ignore_when_grouping={“oven temperature”}</cite>.
That way, two recipes that differ only in their oven temperature will always end up in the same fold.</p>
</div>
</div>
<div class="section" id="predictor-evaluation-metrics">
<h2>4.11.2. Predictor evaluation metrics<a class="headerlink" href="#predictor-evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<p>Predictor evaluation metrics are defined as part of a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.PredictorEvaluator" title="citrine.informatics.predictor_evaluator.PredictorEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluator</span></code></a>.
For all response types, the metric (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PVA" title="citrine.informatics.predictor_evaluation_metrics.PVA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PVA</span></code></a>) is available to compare predicted versus actual data.
Other available metrics depend on whether the response’s type is numeric or categorical.</p>
<p>For numeric responses, the following metrics are available:</p>
<blockquote>
<div><ul class="simple">
<li><p><em>Root-mean squared error</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RMSE" title="citrine.informatics.predictor_evaluation_metrics.RMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">RMSE</span></code></a>): square root of the average of the squared prediction error.
RMSE is a useful and popular statistical metric for model quality.
RMSE is optimized by least-squares regression.
It has the same units as the predicted quantity and corresponds to the standard deviation of the residuals not explained by the predictor.
Lower RMSE means the model is more accurate.</p></li>
<li><p><em>R^2</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RSquared" title="citrine.informatics.predictor_evaluation_metrics.RSquared"><code class="xref py py-class docutils literal notranslate"><span class="pre">RSquared</span></code></a>): 1 - (mean squared error / variance of data).
More precisely known as the “fraction of variance explained,” this metric is equal to the coefficient of determination calculated with respect to the line <code class="docutils literal notranslate"><span class="pre">predicted</span> <span class="pre">=</span> <span class="pre">actual</span></code>.
Hence it is commonly referred to as “R^2,” but unlike R^2 in the context of linear regression, this metric can be negative.
Positive values mean that the model captures some of the variation across the training data, and it can be used to drive sequential learning.
A value of 1.0 indicates a perfect model.
R^2 is evaluated over all cross-validation folds, hence no uncertainty is calculated for the metric, though the value will vary slightly if cross validation is re-run.</p></li>
<li><p><em>Non-dimensional error</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.NDME" title="citrine.informatics.predictor_evaluation_metrics.NDME"><code class="xref py py-class docutils literal notranslate"><span class="pre">NDME</span></code></a>): RMSE divided by the standard deviation of the observed values in the test set.
(If training and test set are drawn from the same distribution, the standard deviation of the test set observed values is equivalent to the RMSE of a model that always predicts the mean of the observed values).
NDME is a useful non-dimensional model quality metric.
A value of NDME == 0 is a perfect model.
If NDME == 1, then the model is uninformative.
Generally, models with NDME &lt; 0.9 can be used in a design workflow.</p></li>
<li><p><em>Standard residual</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.StandardRMSE" title="citrine.informatics.predictor_evaluation_metrics.StandardRMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardRMSE</span></code></a>) is the root mean square of standardized errors (prediction errors divided by their predicted uncertainty).
1.0 is perfectly calibrated.
Standard residual provides a way to determine whether uncertainty estimates are well-calibrated for this model.
Residuals are calculated using <code class="docutils literal notranslate"><span class="pre">(Predicted</span> <span class="pre">-</span> <span class="pre">Actual)/(Uncertainty</span> <span class="pre">Estimate)</span></code>.
A value below 1 indicates the model is underconfident, i.e. actual values are within predicted error bars, on average.
A value over 1 indicates the model is overconfident, i.e. actual values fall outside predicted error bars, on average.</p></li>
<li><p><em>Coverage probability</em> (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.CoverageProbability" title="citrine.informatics.predictor_evaluation_metrics.CoverageProbability"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoverageProbability</span></code></a>) is the fraction of observations for which the magnitude of the error is within a confidence interval of a given coverage level.
The default coverage level is 0.683, corresponding to one standard deviation.
The coverage level and coverage probability must both be between 0 and 1.0.
If the coverage probability is greater than the coverage level then the model is under-confident, and if the coverage probability is less than the coverage level then the model is over-confident.
While standard residual is weighted towards the outside of the residual distribution (because it looks like a 2-norm), coverage probability gives information about the center of the residual distribution.</p></li>
</ul>
</div></blockquote>
<p>For categorical responses, performance metrics include either the area under the receiver operating characteristic (ROC) curve (if there are 2 categories) or the F1 score (if there are &gt; 2 categories).</p>
<ul class="simple">
<li><p>Area under the ROC curve (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.AreaUnderROC" title="citrine.informatics.predictor_evaluation_metrics.AreaUnderROC"><code class="xref py py-class docutils literal notranslate"><span class="pre">AreaUnderROC</span></code></a>) represents the ability of the model to correctly distinguish samples between two categories.
If AUC == 1.0, all samples are classified correctly.
If AUC == 0.5, the model cannot distinguish between the two categories.
If AUC == 0.0, all samples are classified incorrectly.</p></li>
<li><p>Support-weighted F1 score (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.F1" title="citrine.informatics.predictor_evaluation_metrics.F1"><code class="xref py py-class docutils literal notranslate"><span class="pre">F1</span></code></a>) is calculated from averaged precision and recall of the model, weighted by the in-class fraction of true positives according to the formula <code class="docutils literal notranslate"><span class="pre">2.0</span> <span class="pre">*</span> <span class="pre">precision</span> <span class="pre">*</span> <span class="pre">recall</span> <span class="pre">/</span> <span class="pre">(precision</span> <span class="pre">+</span> <span class="pre">recall)</span> <span class="pre">*</span> <span class="pre">fraction_true_positives</span></code> summed over each class.
Scores are bounded by 0 and 1.
At a value of 1, the model has perfect precision and recall.</p></li>
</ul>
</div>
<div class="section" id="execution-and-results">
<span id="id1"></span><h2>4.11.3. Execution and results<a class="headerlink" href="#execution-and-results" title="Permalink to this headline">¶</a></h2>
<p>Triggering a Predictor Evaluation Workflow produces a <a class="reference internal" href="../reference/citrine.resources.predictor_evaluation_execution.html#citrine.resources.predictor_evaluation_execution.PredictorEvaluationExecution" title="citrine.resources.predictor_evaluation_execution.PredictorEvaluationExecution"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationExecution</span></code></a>.
This execution allows you to track the progress using its <code class="docutils literal notranslate"><span class="pre">status</span></code> and <code class="docutils literal notranslate"><span class="pre">status_info</span></code> properties.
The <code class="docutils literal notranslate"><span class="pre">status</span></code> can be one of <code class="docutils literal notranslate"><span class="pre">INPROGRESS</span></code>, <code class="docutils literal notranslate"><span class="pre">READY</span></code> or <code class="docutils literal notranslate"><span class="pre">FAILED</span></code>.
Information about the execution status, e.g., warnings or reasons for failure, can be accessed via <code class="docutils literal notranslate"><span class="pre">status_info</span></code>.</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">status</span></code> is <code class="docutils literal notranslate"><span class="pre">READY</span></code>, results for each evaluation defined as part of the workflow can be accessed using the <code class="docutils literal notranslate"><span class="pre">results</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">execution</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="s1">&#39;evaluator_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>or by indexing into the execution object directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">execution</span><span class="p">[</span><span class="s1">&#39;evaluator_name&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Both methods return a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.PredictorEvaluationResult" title="citrine.informatics.predictor_evaluation_result.PredictorEvaluationResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationResult</span></code></a>.</p>
<p>Each evaluator defines its own result.
A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> returns a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.CrossValidationResult" title="citrine.informatics.predictor_evaluation_result.CrossValidationResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationResult</span></code></a>, for example.
All predictor evaluation results contain a reference to the evaluator that created the result, the set of responses that were evaluated and the metrics that were computed.</p>
<p>Values associated with computed metrics can be accessed by response key:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response_metrics</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;response_key&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>This returns a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.ResponseMetrics" title="citrine.informatics.predictor_evaluation_result.ResponseMetrics"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResponseMetrics</span></code></a> object.
This object contains all metrics that were computed for the <code class="docutils literal notranslate"><span class="pre">response_key</span></code>.
These metrics can be listed using <code class="docutils literal notranslate"><span class="pre">list(response_metrics)</span></code>,
and the value associated with a specific metric can be accessed by the metric itself, e.g., <code class="docutils literal notranslate"><span class="pre">response_metrics[RMSE()]</span></code> to retrieve the root-mean squared error.</p>
<p>With the exception of predicted vs. actual data, all metric values are returned as a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.RealMetricValue" title="citrine.informatics.predictor_evaluation_result.RealMetricValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">RealMetricValue</span></code></a>.
This object defines properties <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">standard_error</span></code>.
The latter optionally returns a float if the evaluation was configured with enough trials allow <code class="docutils literal notranslate"><span class="pre">standard_error</span></code> to be computed.
(A <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a> requires at least 3 trials to compute <code class="docutils literal notranslate"><span class="pre">standard_error</span></code>.)</p>
<p>Predicted vs. actual data (<code class="docutils literal notranslate"><span class="pre">response_metrics[PVA()]</span></code>) is returned as a list of predicted vs. actual data points.
Each data point defines properties <code class="docutils literal notranslate"><span class="pre">uuid</span></code>, <code class="docutils literal notranslate"><span class="pre">identifiers</span></code>, <code class="docutils literal notranslate"><span class="pre">trial</span></code>, <code class="docutils literal notranslate"><span class="pre">fold</span></code>, <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">uuid</span></code> and <code class="docutils literal notranslate"><span class="pre">identifiers</span></code> allow you to link a predicted vs. actual data point to the corresponding row in the <a class="reference internal" href="predictors.html#predictors"><span class="std std-ref">Predictor</span></a>’s <a class="reference internal" href="data_sources.html#data-sources"><span class="std std-ref">Data Source</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trial</span></code> and <code class="docutils literal notranslate"><span class="pre">fold</span></code> return the respective index assigned during the evaluation.</p></li>
<li><p>The form of <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code> data depends on whether the response is numeric or categorical.
For numeric responses, <code class="docutils literal notranslate"><span class="pre">predicted</span></code> and <code class="docutils literal notranslate"><span class="pre">actual</span></code> return a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_result.html#citrine.informatics.predictor_evaluation_result.RealMetricValue" title="citrine.informatics.predictor_evaluation_result.RealMetricValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">RealMetricValue</span></code></a> which reports mean and standard error associated the data point.
For categorical responses, class probabilities are returned as a mapping from each class name (as a string) to its relative frequency (as a float).</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="example">
<h2>4.11.4. Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>The following demonstrates how to create a <a class="reference internal" href="../reference/citrine.informatics.predictor_evaluator.html#citrine.informatics.predictor_evaluator.CrossValidationEvaluator" title="citrine.informatics.predictor_evaluator.CrossValidationEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossValidationEvaluator</span></code></a>, add it to a <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a> and use it to evaluate a <a class="reference internal" href="../reference/citrine.informatics.predictors.predictor.html#citrine.informatics.predictors.predictor.Predictor" title="citrine.informatics.predictors.predictor.Predictor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Predictor</span></code></a>.</p>
<p>The predictor we’ll evaluate is defined below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.data_sources</span> <span class="kn">import</span> <span class="n">CSVDataSource</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.descriptors</span> <span class="kn">import</span> <span class="n">RealDescriptor</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.predictors</span> <span class="kn">import</span> <span class="n">SimpleMLPredictor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">RealDescriptor</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">RealDescriptor</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">data_source</span> <span class="o">=</span> <span class="n">CSVDataSource</span><span class="p">(</span>
    <span class="n">file_link</span><span class="o">=</span><span class="n">file</span><span class="p">,</span> <span class="c1"># path to CSV that contains training data for x and y</span>
    <span class="n">column_definitions</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">SimpleMLPredictor</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y predictor&#39;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s1">&#39;predicts y given x&#39;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span>
    <span class="n">latent_variables</span><span class="o">=</span><span class="p">[],</span>
    <span class="n">training_data</span><span class="o">=</span><span class="p">[</span><span class="n">data_source</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This predictor expects <code class="docutils literal notranslate"><span class="pre">x</span></code> as an input and predicts <code class="docutils literal notranslate"><span class="pre">y</span></code>.
Training data is provided by a <a class="reference internal" href="../reference/citrine.informatics.data_sources.html#citrine.informatics.data_sources.CSVDataSource" title="citrine.informatics.data_sources.CSVDataSource"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVDataSource</span></code></a> that assumes <code class="docutils literal notranslate"><span class="pre">filename</span></code> represents the path to a CSV that contains <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>Next, create a project and register the predictor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">citrine.jobs.waiting</span> <span class="kn">import</span> <span class="n">wait_while_validating</span>
<span class="kn">from</span> <span class="nn">citrine.seeding.find_or_create</span> <span class="kn">import</span> <span class="n">find_or_create_project</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Citrine</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;CITRINE_API_KEY&#39;</span><span class="p">))</span>
<span class="n">project</span> <span class="o">=</span> <span class="n">find_or_create_project</span><span class="p">(</span><span class="n">project_collection</span><span class="o">=</span><span class="n">client</span><span class="o">.</span><span class="n">projects</span><span class="p">,</span> <span class="n">project_name</span><span class="o">=</span><span class="s1">&#39;example project&#39;</span><span class="p">)</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">project</span><span class="o">.</span><span class="n">predictors</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
<span class="n">wait_while_validating</span><span class="p">(</span><span class="n">collection</span><span class="o">=</span><span class="n">project</span><span class="o">.</span><span class="n">predictors</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">predictor</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example we’ll create a cross-validation evaluator for the response <code class="docutils literal notranslate"><span class="pre">y</span></code> with 8 folds and 3 trials and request metrics for root-mean square error (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.RMSE" title="citrine.informatics.predictor_evaluation_metrics.RMSE"><code class="xref py py-class docutils literal notranslate"><span class="pre">RMSE</span></code></a>) and predicted vs. actual data (<a class="reference internal" href="../reference/citrine.informatics.predictor_evaluation_metrics.html#citrine.informatics.predictor_evaluation_metrics.PVA" title="citrine.informatics.predictor_evaluation_metrics.PVA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PVA</span></code></a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here we’re performing cross-validation on an output, but latent variables are valid cross-validation responses as well.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.predictor_evaluator</span> <span class="kn">import</span> <span class="n">CrossValidationEvaluator</span>
<span class="kn">from</span> <span class="nn">citrine.informatics.predictor_evaluation_metrics</span> <span class="kn">import</span> <span class="n">RMSE</span><span class="p">,</span> <span class="n">PVA</span>

<span class="n">evaluator</span> <span class="o">=</span> <span class="n">CrossValidationEvaluator</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cv&#39;</span><span class="p">,</span>
    <span class="n">n_folds</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">responses</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;y&#39;</span><span class="p">},</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="n">RMSE</span><span class="p">(),</span> <span class="n">PVA</span><span class="p">()}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Then add the evaluator to a <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflow</span></code></a>, register it with your project and wait for validation to finish:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.informatics.workflows</span> <span class="kn">import</span> <span class="n">PredictorEvaluationWorkflow</span>

<span class="n">workflow</span> <span class="o">=</span> <span class="n">PredictorEvaluationWorkflow</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;workflow that evaluates y&#39;</span><span class="p">,</span>
    <span class="n">evaluators</span><span class="o">=</span><span class="p">[</span><span class="n">evaluator</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">workflow</span> <span class="o">=</span> <span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">workflow</span><span class="p">)</span>
<span class="n">wait_while_validating</span><span class="p">(</span><span class="n">collection</span><span class="o">=</span><span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">workflow</span><span class="p">)</span>
</pre></div>
</div>
<p>Trigger the workflow against a predictor to start an execution.
Then wait for the results to be ready:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">citrine.jobs.waiting</span> <span class="kn">import</span> <span class="n">wait_while_executing</span>

<span class="n">execution</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">executions</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
<span class="n">wait_while_executing</span><span class="p">(</span><span class="n">execution</span><span class="p">,</span> <span class="n">print_status_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collection</span> <span class="o">=</span> <span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_executions</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, load the results and inspect the metrics and their computed values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the results computed by the CV evaluator defined above</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">execution</span><span class="p">[</span><span class="n">evaluator</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

<span class="c1"># load results for y</span>
<span class="n">y_results</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>

<span class="c1"># listing the results should return the metrics we requested: RMSE and PVA</span>
<span class="n">computed_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">computed_metrics</span><span class="p">)</span> <span class="c1"># [&#39;rmse&#39;, &#39;predicted_vs_actual&#39;]</span>

<span class="c1"># access RMSE and print the mean and standard error</span>
<span class="n">y_rmse</span> <span class="o">=</span> <span class="n">y_results</span><span class="p">[</span><span class="n">RMSE</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;RMSE: mean = </span><span class="si">{</span><span class="n">y_rmse</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">, standard error = </span><span class="si">{</span><span class="n">y_rmse</span><span class="o">.</span><span class="n">standard_error</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># access PVA:</span>
<span class="n">y_pva</span> <span class="o">=</span> <span class="n">y_results</span><span class="p">[</span><span class="n">PVA</span><span class="p">()]</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pva</span><span class="p">))</span> <span class="c1"># this should equal the num_trials * num_folds * num_rows</span>
                  <span class="c1"># where num_rows == the number of rows in the data source</span>

<span class="c1"># inspect the first data point</span>
<span class="n">pva_data_point</span> <span class="o">=</span> <span class="n">y_pva</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># print trial and fold indices</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pva_data_point</span><span class="o">.</span><span class="n">trial</span><span class="p">)</span> <span class="c1"># should be == 1 since trials are 1-indexed,</span>
                            <span class="c1"># and this it the first data point</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pva_data_point</span><span class="o">.</span><span class="n">fold</span><span class="p">)</span> <span class="c1"># should also be == 1</span>

<span class="c1"># inspect predicted and actual values</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">pva_data_point</span><span class="o">.</span><span class="n">predicted</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted = </span><span class="si">{</span><span class="n">predicted</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="n">predicted</span><span class="o">.</span><span class="n">standard_error</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">pva_data_point</span><span class="o">.</span><span class="n">actual</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;actual = </span><span class="si">{</span><span class="n">actual</span><span class="o">.</span><span class="n">mean</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="n">actual</span><span class="o">.</span><span class="n">standard_error</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="archive-and-restore">
<h2>4.11.5. Archive and restore<a class="headerlink" href="#archive-and-restore" title="Permalink to this headline">¶</a></h2>
<p>Both <a class="reference internal" href="../reference/citrine.informatics.workflows.predictor_evaluation_workflow.html#citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow" title="citrine.informatics.workflows.predictor_evaluation_workflow.PredictorEvaluationWorkflow"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationWorkflows</span></code></a> and <a class="reference internal" href="../reference/citrine.resources.predictor_evaluation_execution.html#citrine.resources.predictor_evaluation_execution.PredictorEvaluationExecution" title="citrine.resources.predictor_evaluation_execution.PredictorEvaluationExecution"><code class="xref py py-class docutils literal notranslate"><span class="pre">PredictorEvaluationExecutions</span></code></a> can be archived and restored.
To archive a workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_workflows</span><span class="o">.</span><span class="n">archive</span><span class="p">(</span><span class="n">workflow</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
</pre></div>
</div>
<p>and to archive all executions associated with a workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">execution</span> <span class="ow">in</span> <span class="n">workflow</span><span class="o">.</span><span class="n">executions</span><span class="o">.</span><span class="n">list</span><span class="p">():</span>
    <span class="n">project</span><span class="o">.</span><span class="n">predictor_evaluation_executions</span><span class="o">.</span><span class="n">archive</span><span class="p">(</span><span class="n">execution</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
</pre></div>
</div>
<p>To restore a workflow or execution, simply replace <code class="docutils literal notranslate"><span class="pre">archive</span></code> with <code class="docutils literal notranslate"><span class="pre">restore</span></code> in the code above.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=CitrineInformatics&repo=citrine-python&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">1. Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/overview.html">1.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/connecting.html">1.2. Connecting to the Citrine Platform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/basic_functionality.html">1.3. Managing Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/projects.html">1.4. Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/datasets.html">1.5. Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/data_model.html">1.6. Data Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/ai_modules.html">1.7. AI Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/code_examples.html">1.8. Code Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data_entry.html">2. Data Entry</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#creating-data-model-objects">2.1. Creating Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#identifying-data-model-objects">2.2. Identifying Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#registering-data-model-objects">2.3. Registering Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#finding-data-model-objects">2.4. Finding Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#updating-data-model-objects">2.5. Updating Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#referencing-data-model-objects">2.6. Referencing Data Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#material-history">2.7. Material History</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_entry.html#validating-data-model-objects">2.8. Validating Data Model Objects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data_extraction.html">3. [ALPHA] Data Extraction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#defining-row-and-columns">3.1. Defining row and columns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#defining-tables">3.2. Defining tables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#previewing-tables">3.3. Previewing tables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#building-and-downloading-tables">3.4. Building and downloading tables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#available-row-definitions">3.5. Available Row Definitions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#available-variable-definitions">3.6. Available Variable Definitions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_extraction.html#available-column-definitions">3.7. Available Column Definitions</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">4. AI Engine</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">4.1. Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="descriptors.html">4.2. Descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="descriptors.html#platform-vocabularies">4.3. Platform Vocabularies</a></li>
<li class="toctree-l2"><a class="reference internal" href="predictors.html">4.4. Predictors</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_spaces.html">4.5. Design Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="processors.html">4.6. Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_workflows.html">4.7. Design Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="scores.html">4.8. Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_sources.html">4.9. Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="predictor_reports.html">4.10. Predictor Reports</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.11. Predictor Evaluation Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_workflows.html">4.12. [DEPRECATED] Performance Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="code_examples.html">4.13. AI Engine Code Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ/index.html">5. FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../FAQ/prohibited_data_patterns.html">5.1. Prohibited Data Patterns</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">4. AI Engine</a><ul>
      <li>Previous: <a href="predictor_reports.html" title="previous chapter">4.10. Predictor Reports</a></li>
      <li>Next: <a href="performance_workflows.html" title="next chapter">4.12. [DEPRECATED] Performance Workflows</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Citrine Informatics.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/workflows/predictor_evaluation_workflows.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>